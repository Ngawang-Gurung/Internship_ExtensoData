{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.1\n"
     ]
    }
   ],
   "source": [
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Creating df from a list of rows\n",
    "\n",
    "# from datetime import datetime, date\n",
    "# from pyspark.sql import Row\n",
    "\n",
    "# df = spark.createDataFrame([\n",
    "#     Row(a = 1, b = 2., c='string1', d = date(2000, 1, 1)),\n",
    "#     Row(a = 2, b = 3., c='string2', d = date(2000, 2, 1)),\n",
    "# ])\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Creating df with an explicit schema\n",
    "\n",
    "# df = spark.createDataFrame([\n",
    "#     (1, 2., 'string1', date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),\n",
    "#     (2, 3., 'string2', date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),\n",
    "#     (3, 4., 'string3', date(2000, 3, 1), datetime(2000, 1, 3, 12, 0))\n",
    "# ], schema='a long, b double, c string, d date, e timestamp')\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating spark dataframe from a pandas dataframe\n",
    "\n",
    "# import pandas as pd\n",
    "# from datetime import datetime, date\n",
    "\n",
    "# pandas_df = pd.DataFrame({\n",
    "#     'a': [1, 2, 3],\n",
    "#     'b': [2., 3., 4.],\n",
    "#     'c': ['string1', 'string2', 'string3'],\n",
    "#     'd': [date(2000, 1, 1), date(2000, 2, 1), date(2000, 3, 1)],\n",
    "#     'e': [datetime(2000, 1, 1, 12, 0), datetime(2000, 1, 2, 12, 0), datetime(2000, 1, 3, 12, 0)]\n",
    "# })\n",
    "# df = spark.createDataFrame(pandas_df)\n",
    "# type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating pandas-on-Spark Dataframe\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import pyspark.pandas as ps\n",
    "\n",
    "\n",
    "# pdf = pd.DataFrame(np.random.randn(6, 4), columns=list('ABCD'))\n",
    "# psdf = ps.from_pandas(pdf)\n",
    "# type(psdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geting Data In/Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('OnlineRetail.csv', header = True)\n",
    "\n",
    "# df.write.csv('OnlineRetail.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/2010 8:26|     2.75|     17850|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.conf.set('spark.sql.repl.eagerEval.enabled', True)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['InvoiceNo',\n",
       " 'StockCode',\n",
       " 'Description',\n",
       " 'Quantity',\n",
       " 'InvoiceDate',\n",
       " 'UnitPrice',\n",
       " 'CustomerID',\n",
       " 'Country']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: string (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: string (nullable = true)\n",
      " |-- CustomerID: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+--------------------+------------------+---------------+------------------+------------------+-----------+\n",
      "|summary|         InvoiceNo|         StockCode|         Description|          Quantity|    InvoiceDate|         UnitPrice|        CustomerID|    Country|\n",
      "+-------+------------------+------------------+--------------------+------------------+---------------+------------------+------------------+-----------+\n",
      "|  count|            541909|            541909|              540455|            541909|         541909|            541909|            406829|     541909|\n",
      "|   mean|  559965.752026781|27623.240210938104|             20713.0|  9.55224954743324|           NULL|4.6111136260897085|15287.690570239585|       NULL|\n",
      "| stddev|13428.417280796779|16799.737628427658|                NULL|218.08115785023438|           NULL| 96.75985306117963|1713.6003033215982|       NULL|\n",
      "|    min|            536365|             10002| 4 PURPLE FLOCK D...|                -1|1/10/2011 10:04|         -11062.06|             12346|  Australia|\n",
      "|    max|           C581569|                 m|   wrongly sold sets|               992|  9/9/2011 9:52|             99.96|             18287|Unspecified|\n",
      "+-------+------------------+------------------+--------------------+------------------+---------------+------------------+------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Summary of DataFrame\n",
    "\n",
    "df.select(\"*\").describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion to Pandas DataFrame\n",
    "\n",
    "# df.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting and Accessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'InvoiceNo'>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PySpark DataFrame is lazily evaluated and simply selecting a column does not \n",
    "# trigger the computation but it returns a Column instance.\n",
    "df.InvoiceNo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|InvoiceNo|\n",
      "+---------+\n",
      "|   536365|\n",
      "|   536365|\n",
      "|   536365|\n",
      "|   536365|\n",
      "|   536365|\n",
      "+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  DataFrame.select() takes the Column instances that returns another DataFrame.\n",
    "\n",
    "df.select(df.InvoiceNo).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|         Upper|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|UNITED KINGDOM|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|UNITED KINGDOM|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/2010 8:26|     2.75|     17850|United Kingdom|UNITED KINGDOM|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|UNITED KINGDOM|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|UNITED KINGDOM|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To assign new column instance\n",
    "\n",
    "from pyspark.sql.functions import upper\n",
    "df.withColumn('Upper', upper(df.Country)).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+---------------+---------+----------+-------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|    InvoiceDate|UnitPrice|CustomerID|Country|\n",
      "+---------+---------+--------------------+--------+---------------+---------+----------+-------+\n",
      "|   536527|    22809|SET OF 6 T-LIGHTS...|       6|12/1/2010 13:04|     2.95|     12662|Germany|\n",
      "|   536527|    84347|ROTATING SILVER A...|       6|12/1/2010 13:04|     2.55|     12662|Germany|\n",
      "|   536527|    84945|MULTI COLOUR SILV...|      12|12/1/2010 13:04|     0.85|     12662|Germany|\n",
      "|   536527|    22242|5 HOOK HANGER MAG...|      12|12/1/2010 13:04|     1.65|     12662|Germany|\n",
      "|   536527|    22244|3 HOOK HANGER MAG...|      12|12/1/2010 13:04|     1.95|     12662|Germany|\n",
      "+---------+---------+--------------------+--------+---------------+---------+----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To select a subset of rows\n",
    "\n",
    "df.filter(df.Country == 'Germany').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing Datatype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: string (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: string (nullable = true)\n",
      " |-- CustomerID: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import col\n",
    "df = df.withColumn(\"Quantity\", col(\"Quantity\").cast(\"integer\"))\n",
    "df = df.withColumn(\"UnitPrice\", col(\"UnitPrice\").cast(\"integer\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+\n",
      "|CustomerID|     avg(Quantity)|    avg(UnitPrice)|\n",
      "+----------+------------------+------------------+\n",
      "|     16250| 8.666666666666666|             2.375|\n",
      "|     15574|2.0773809523809526|               2.0|\n",
      "|     15555| 4.767567567567568| 1.172972972972973|\n",
      "|     15271| 4.570909090909091|              2.12|\n",
      "|     17714|               9.2|               1.6|\n",
      "|     17757|  4.46900269541779|2.1361185983827493|\n",
      "|     17551| 3.488372093023256|1.9767441860465116|\n",
      "|     13187|2.5675675675675675| 3.189189189189189|\n",
      "|     16549|2.4108053007135575|1.5168195718654434|\n",
      "|     12637| 7.588832487309645|2.0913705583756346|\n",
      "|     15052|2.7333333333333334|2.8666666666666667|\n",
      "|     14525|  8.93288590604027| 2.661073825503356|\n",
      "|     18283| 1.847883597883598|1.0833333333333333|\n",
      "|     13107|21.466666666666665|1.2833333333333334|\n",
      "|     16303|15.694610778443113|2.2335329341317367|\n",
      "|     13174| 4.442675159235669| 2.767515923566879|\n",
      "|     13027| 664.6153846153846|               0.0|\n",
      "|     12957| 10.37295081967213|              2.25|\n",
      "|     17128| 5.428571428571429|3.2142857142857144|\n",
      "|     14439|           16.4375|           5.78125|\n",
      "+----------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Groupby aggregation works on numeric columns only\n",
    "df.groupby('CustomerID').avg().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with SQL\n",
    "\n",
    "DataFrame and Spark SQL share the same execution engine so they can be interchangeably used seamlessly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  541909|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"tableA\")\n",
    "spark.sql(\"SELECT count(*) from tableA\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
